\documentclass[a4paper,12pt,openright,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{esvect}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{url}

\theoremstyle{definition}
\newtheorem{es}{Esempio}[section]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%comandi per l'impostazione
                                        %   della pagina, vedi il manuale
                                        %   della libreria fancyhdr
                                        %   per ulteriori delucidazioni
\pagestyle{fancy}\addtolength{\headwidth}{20pt}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection \ #1}{}}
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\cfoot{}

\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else
\hbox{}
\vspace*{\fill}
\begin{center}
\textit{ }
\end{center}
\vspace{\fill}
\thispagestyle{empty}
\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linespread{1.2}                        %comando per impostare l'interlinea
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%definisce nuovi comandi

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
\clearpage{\pagestyle{empty}\cleardoublepage}
\tableofcontents                        %crea l'indice
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
Indice}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
%\clearpage{\pagestyle{empty}\cleardoublepage}
%\listoffigures                          %crea l'elenco delle figure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
%\clearpage{\pagestyle{empty}\cleardoublepage}
%\listoftables                           %crea l'elenco delle tabelle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
%\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}

\chapter{Introduzione}

\section{Sentiment Analisys}
\label{sec:sentAnal}
Con il termine \emph{Sentiment analysis}, o \emph{opinion minig}, si intende uno studio 
che analizza le opinioni, i sentimenti, le valutazioni e le
emozioni del linguaggio scritto utilizzato dalle persone.
Spesso il concetto di \emph{sentiment analysis} si estende anche all'ambito del \emph{data mining} e
del \emph{text mining}, ossia quella branca di ricerca riguardante l'estrazione di un sapere o 
di una conoscenza a partire da grandi quantità di dati.

La \emph{Sentiment analysis}, nata nell'ambito dell'informatica, è una delle aree di ricerca più attive 
nel campo dell'analisi del linguaggio naturale e si è diffusa
ampiamente anche in altri rami scientifici come ad esempio: la scienza sociale,
l'economia e il marketing. Inoltre questo tipo di ricerca
ha avuto un grande impatto sia a livello commerciale, per aziende e multinazionali, che sociologico, 
coinvolgendo l'intera società che ci circonda.

L'enorme diffusione della \emph{sentiment analysis} coincide con la 
crescita dei cosiddetti \emph{social media}: siti di commercio e recensioni di prodotti,
forum di discussione, blog, micro-blog e di vari \emph{social network}.
Per la prima volta nella storia è possibile disporre di una
enorme quantità di dati digitalizzati. Dati da manipolare, analizzare e studiare in maniera
estremamente approfondita, al fine di estrapolare ed ottenere un'analisi
ampiamente sfaccettata e particolareggiata.

Gli studi di \emph{sentiment analysis} sono applicabili in quasi tutte
le attività del cosiddetto dominio sociale, le opinioni e i pareri 
sono infatti al centro della maggior parte delle attività umane e sono 
altresì la chiave che influenza il nostro comportamento.
Ciò in cui crediamo, la percezione della realtà, le scelte che facciamo
sono condizionate dall’ambiente che ci circonda e dai giudizi 
degli altri riguardo noi stessi.
Per questo motivo, quando abbiamo bisogno
di prendere una decisione, spesso chiediamo il parere e il consenso altrui.
Questo comportamento si verifica non solo 
nei singoli individui, ma lo possiamo  vedere applicato allo stesso modo ad
applicato ad organizzazioni, aziende e grandi multinazionali.
Da tutti questi aspetti è facile dedurre 
come, \emph{la sentiment analysis} possa avere una grande influenza nel mondo che 
ci circonda sia dal punto di vista economico che sociale.

Mentre la linguistica e l'analisi del linguaggio naturale (NLP) hanno una lunga
storia, la \emph{sentiment analysis} è relativamente recente: le prime ricerche
sono state svolte a partire dall'anno 2000. Nonostante questo ambito di ricerca sia così giovane 
si presenta come un'area estremamente attiva.
La crescita della ricerca nel campo della \emph{sentiment analysis}, avvenuta in maniera così immediata e su larga scala, 
ha molteplici ragioni. 
Per prima cosa, le applicazioni pratiche 
di questo tipo di ricerca sono svariate e possono essere utilizzate in un ampio raggio di 
situazioni e domini differenti. Ciò ha generato un aumento di interesse notevole da parte di
aziende e di grandi multinazionali (non solo in ambito informatico)
con un relativo stanziamento di finanziamenti specifici per questo campo di ricerca.
In secondo luogo, il grande interesse dimostrato da parte del mercato ha portato ad una proliferazione di 
applicazioni commerciali e ad una costante ricerca della soluzione di problemi
che fino ad ora non erano ancora stati studiati.

\subsection{Applicazioni della Sentiment Analysis}
Partiamo dunque dal presupposto che le opinioni e i pareri siano la base di tutte le attività sociali,
 essendo la chiave che influenza il nostro comportamento. Nel mondo reale, aziende e multinazionali 
 sono costantemente alla ricerca di giudizi e pareri dati dai consumatori  rispetto ai loro prodotti. 
 Allo stesso modo il consumatore vuole avere informazioni che riguardino gli articoli da acquistare, 
 opinioni inerenti un determinato film o un nuovo album musicale e così via. In passato, quando un’azienda 
 necessitava di un parere del consumatore, effettuava sondaggi d’opinione o gruppi di discussione: 
 ottenere questo tipo di dati è stato per lungo tempo un grande business per aziende di marketing e 
 per compagnie pubblicitarie.
Al giorno d’oggi chiunque voglia comprare un prodotto, potendo trovare centinaia di recensioni o 
discussioni online non è più limitato a dover chiedere informazioni ad amici o famigliari. 
Allo stesso modo le aziende non sono più costrette a condurre sondaggi o analisi di mercato vista 
l’enorme diffusione dei dati disponibili. Tuttavia la ricerca, l’estrazione e l’annotazione di questi 
dati è un compito di una complessità non indifferente, basti pensare alla quantità di informazioni 
scritte nel post di un blog o in un forum, che rendono difficoltosa anche ad un singolo l'elaborazione 
e la sintesi delle informazioni utili: ecco perché diventa cosi importante uno studio finalizzato e 
strutturato in un determinato ambito.

\chapter{Modelli utilizzati}

Lo scopo di questo capitolo è quello di analizzare alcune ricerche effettuate sulla 
\emph{sentiment analysis}, in particolare quelle in cui sono presenti gli stessi strumenti 
utilizzati in questa tesi: le reti neurali ricorsive (\emph{RNNLM}), 
la rappresentazione vettoriale di parole (\emph{word embedding})
con (\emph{word2vec}) e 
i classificatori lineari (\emph{Liblinear}).

Come già esposto nel Capitolo \ref{sec:sentAnal} e come si può desumere
dalle ricerche effettuate da Pang and Lee (2008) %todo
la \emph{sentiment classification} è uno degli aspetti più studiati della NLP.
L’obiettivo che questo tipo di analisi si prefigge è quello di operare una classificazione su determinati testi, 
suddividendoli in base ai contenuti, positivi oppure negativi, che gli stessi esprimono. 

Una ricerca condotta da 
Liu (2010) %todo
ha portato a teorizzare due diverse modalità per affrontare questo problema: la prima, denominata
 \emph{classification}
categorizza il testo in base a due valori differenti (es. positivo o negativo); la seconda, definita 
\emph{regression}, invece assegna un punteggio ai testi analizzati (es. un valore da 1 a 5)
in modo tale che la loro differenziazione si basi su un maggior numero di elementi discriminanti.

Entrando nello specifico, possiamo constatare come il lavoro di Liu et al. (2007) %todo
proponga una \emph{sentiment analisys} per ipotizzare le performance di vendita di una particolare
azienda. 
\`E interessante, inoltre, soffermarsi sulle numerose ricerche svolte nell’ambito dei social network, 
settore che oltretutto è più vicino all’argomento trattato in questa tesi: ricordiamo, in primo luogo,  lo studio 
Yano e Smith (2010) %todo
che presenta un metodo per prevedere l'esito di film, analizzando i dati
provenienti dai forum e dal sito web \emph{Internet Movie Database}; è indicativo, inoltre,
il lavoro di
Groh and Hauffa (2011) %todo
che analizza l’evolversi delle relazioni sociali tra utenti di Twitter.
%todo aggiunge robe relative alla polarity}

Un ulteriore e interessante contributo a questo argomento viene fornito 
dalla pubblicazione di \cite{Ens2015}, dove vengono esaminati vari
approcci di \emph{machine learning} al problema di \emph{sentiment analysis},
in particolare viene presentato l'approccio tradizionale, che utilizza tecniche di
\emph{bag-of-words} o di \emph{bag-of-ngram} (Pang and Lee, 2008; Wang and Manning, 2012)
e, in dettaglio, un approccio più complesso che impiega un \emph{language model} generativo
abbinandolo ad un sistema discriminativo: 
l’utilizzo di quest’ultimo metodo, che usa in maniera simultanea due tipi di modelli complementari
aumenta la precisione del risultato.
La logica sottesa al modello generativo consiste nel fatto che, dopo aver effettuato il \emph{training}
di due modelli su dati che esprimono pareri positivi e pareri negativi,
si utilizza la percentuale di somiglianza di questi due modelli rispetto ai dati del file di
test. In questo modo 
possiamo dunque ipotizzare che sia maggiore la possibilità che uno scritto connotato 
da parere positivo tra quelli utilizzati come test, venga generato da un modello proveniente 
da un set di dati che esprimono pareri positivi rispetto a quelli che esprimono pareri negativi.

In questa tesi è stato utilizzato proprio questo approccio, 
e cioè l’impiego di un sistema generativo in concomitanza con un sistema discriminativo:
in particolare, come modello generativo  è stata usata una \emph{recurrent neural networks} 
\- e nello specifico RNNLM \- e, come modello discriminativo \emph{word2vec}.

Alcune ricerche svolte da \cite{Miktesi} sull'argomento ci confermano che 
gli \emph{statistical language models} sono ampiamente utilizzati
in diverse applicazioni, come per esempio la \emph{speech recognition} o \emph{machine translator}.
Tradizionalmente le tecniche per la costruzione di questi modelli si basano sull'utilizzo di 
N-grammi che,
pur avendo evidenziato alcune debolezze e nonostante l'enorme sforzo della
comunità di ricerca in numerosi campi (\emph{speech recognition}, neuroscienza, intelligenza
artificiale) per trovare un modello alternativo sono rimasti per lungo tempo come lo stato dell'arte.
Recentemente, tuttavia, è stato possibile costruire \emph{Language Models}(LM) 
tramite \emph{Recurrent Neural Network}(RNN), come mostrado da \cite{Mik2010,MikEmp},
è stato altresì dimostrato come queste RNN LM abbiano una qualità superiore rispetto alle tecniche
che utilzzano \emph{n\-grams} anche se ciò comporta una più alta complessità computazionale.

Proprio analizzando questo modello basato su RNN, per la precisione quello
proposto nel lavoro pubblicato da \cite{MikRNN}, appare evidente come sia
stato possibile ridurre la complessità  di calcolo e realizzare RNN LM con
velocità superiori (anche fino a quindici volte) rispetto a RNN base attraverso l'utilizzo
di un algoritmo di \emph{backpropagation throught time}.

Per quanto riguarda il modello discriminativo, in letteratura possiamo vedere come il lavoro di \cite{Baroni2010}
dimostri in che modo una rappresentazione di parole distribuita, 
basata sulla \emph{Harris distributional hypotesis},
abbia giocato un ruolo centrale nel campo dell'analisi del linguaggio naturale (NLP)
più recentemente in vari studi proposti da 
Bengio et al, Collobert and Weston, Mnih e Mikolov et al. (2011) %todo
%\cite{Collobert2011} 
sono stati utilizzati vettori densi derivati da RNN. Questi vettori vengono più specificatamente
definiti \emph{word embedding} e sono stati adottati in numerosi tasks NLP.

Il lavoro presentato è basato sulle ricerche effettuate da Mikolov et al (2013) %todo
che presentano due differenti modelli di \emph{word embedding}, CBOW e 
skip-gram trattati più approfonditamente nel paragrafo \ref{sec:word2vec}:
entrambi i modelli,
attraverso l'uso della regressione lineare, prevedono la parola target.
In particolare, come presentato nella ricerca di Le e Mikolov (2014)%todo
è stato impiegato
un modello che utilizza i \emph{paraghrap vectors}(PV): il modello PV si basa sul fatto
di estendere \emph{word2vec} in modo da catturare informazioni sintattiche e semantiche 
provenienti da un frammento di testo (paragrafi, frasi, interi testi etc.).
Questa estensione del modello si effettua inserendo un \emph{token}, un codice fittizio,
all'inizio del frammento di testo e applicando gli stessi algoritmi utilizzati 
precedentemente, ma considerando l'intero frammento di testo come contesto del \emph{token}.
Al termine del processo di \emph{training} il vettore di \emph{embedding} corrispondete
al \emph{token} sarà il PV che rappresenta l'intero frammento di testo e potra'
essere usato in task di classificazione.

%In particolare, è stato preso in considerazione il lavoro effettuato da \cite{Mik2014}:
%il sistema proposto dai due studiosi rappresenta un testo, che può essere suddiviso in
%frasi, parole o paragrafi utilizzando un vettore denso per prevedere parole nel documento.

%Come mostrato da (chi?) utilizzando classificatori lineari come Liblinear \cite{rong2015} presentato da 
%Rong et al. è possibile effetuare cosa?

%Il lavoro di tesi si basa dalle ricerche presentate da vari autori: è stato effettuato i training 
%della RNN come è stato presentado da Kalchabrenner et al. (2014) e Kim (2014)%todo
%dove sono state presentate nuove riceche allo stato dell'arte riguardo il task di 
%classificazione riguardo la \emph{sentiment analysis}
%Svariate ricerche, che utilizzano RNN o \emph{deep convulotinal networks} hanno dimostrato
%che il \emph{training} di tali reti.


%\`E opportuno sottolineare che i modelli descritti non utilizzano l'informazione
%sull'ordinamento delle parole (Qui et al., 2014; Lai et al., 2015; Trask et al., 2015)%todo.

\section{Recurrent Neural Networks}
Gli esseri umani non iniziano a pensare da zero: esiste infatti per la mente umana una capacità di 
associare tra loro informazioni passate e recenti che è stata definita \emph{persistenza della memoria.} 
Il lettore di questa tesi, ad esempio, assocerà il significato di ogni parola basandosi su quelle 
lette in passato.
Le reti neurali tradizionali, al contrario, ignorano gli eventi passati. Volendo prendere ad esempio 
la classificazione della successione delle scene di un film, non è possibile che una rete neurale 
possa utilizzare gli eventi passati per classificare quelli correnti.

Per cercare di risolvere questo problema sono state create le \emph{recurrent neural networks (RNN)}:
a differenza delle reti neurali tradizionali le \emph{RNN} hanno dei cicli interni che permettono
all'informazione di persistere.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{Immagini/rnn1.png}
	\caption{Ciclo interno di una RNN}
	\label{fig:rnn1}
\end{figure}

Nella figura \ref{fig:rnn1}, è rappresentata una rete neurale $A$ che riceve in input un valore $x_t$ e genera in output un valore $h_t$.
Un ciclo permette all'informazione di passare da ogni step della rete al successivo. 
In altre parole possiamo immaginare
una \emph{RNN} come molteplici copie della stessa rete, ognuna delle quali passa un messaggio alla rete successiva.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.3]{Immagini/rnn2.png}
	\caption{Esempio di RNN \emph{unfolded}, cioè senza cicli}
	\label{fig:rnn2}
\end{figure}

In sostanza una \emph{RNN} riceve come input un vettore $x$ e genera come output un vettore $y$ ma il cuore della rete
è dato dal fatto che l'output viene influenzato non solo dall'ultimo input acquisito ma anche da tutti gli 
input passati. Se volessimo esprimere questo concetto sotto forma di codice avremo una definizione di \emph{RNN}
che è la seguente:
\begin{verbatim}
rnn = RNN()
y = rnn.step(x)
\end{verbatim}
La \emph{RNN} possiede uno stato interno che viene aggiornato ad ogni esecuzione della procedura \emph{step}.
Per esempio una funzione di \emph{step} potrebbe essere implementata come segue:
\begin{verbatim}
class RNN:
  # ...
  def step(self, x):
    # aggiornamento dello stato interno
    self.h = np.tanh(np.dot(self.W_hh, self.h) 
           + np.dot(self.W_xh, x))
    # calcolo del vettore di output
    y = np.dot(self.W_hy, self.h)
    return y
\end{verbatim}

Si può notare come la \emph{RNN} utilizzi tre matrici: \emph{W\_hh, W\_xh, W\_hy}, l'\emph{hidden state self.h}
e' inizializzato ome un vettore di zeri e la funzione \emph{np.tanh} fornisce come output un valore che oscilla tra $[-1,1]$.
All'interno di questa funzione notiamo due termini, uno che si basa sullo stato dell'input attuale e uno che 
rappresenta il valore dello stato interno precedente. I due termini vengono sommati prima di essere processati dalla 
funzione stessa.
Le matrici sono inizializzate con numeri casuali e la maggior parte di tempo speso durante la fase di
\emph{training} viene utilizzato nella ricerca di valori di queste matrici in modo che 
la rete si comporti nella modo desiderato, cioe' che restituisca in output l'$y$ che vorremmo avere
quando forniamo in input la sequenza $x$.

\subsection{RNNLM}
La rete neurale utilizzata per l'analisi dei dati in questa tesi è il toolkit RNNLM. 
L'architettura utilizzata nel toolkit, mostrata in \ref{fig:rnnlm} viene solitamente chiamata rete di Elman o semplicemente RNN. 
Il \emph{layer} di input usa una rappresentazione $1-a-N$ delle 
parole precedenti $w(t)$ concatenate con il precedente stato dell'hidden layer $s(t - 1)$.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.5\textwidth]{Immagini/rnnlm_rete.png}
      \caption{Esempio di rete neurale ricorsiva}
\label{fig:rnnlm}
\end{figure}

I neuroni dell'\emph{hidden layer} $s(t)$ usano una funzione sigmoidea, cioè una funzione matematica
che produce una curva sigmoide (una curva avente un andamento ad $S$).

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.5\textwidth]{Immagini/sigmoide.png}
      \caption{Esempio di grafico di una funzione sigmoidea}
\label{fig:sigmoide}
\end{figure}

Questo tipo di funzione viene utilizzata per introdurre una non linearità nel modello e per
assicurarsi che determinati  valori rimangano all'interno di specifici intervalli.
Dopo il training della rete, l'output layer $y(t)$, che ha le stesse dimensioni di $w(t)$,
rappresenta la probabilità della distribuzione della parola $n+1$ partendo dalla parola $n$ 
e lo stato dell'\emph{hidden layer} nel precedente istante di tempo.
Un class layer $c(t)$ può essere usato per ridurre la complessità del modello ma con un piccola
diminuzione della precisione.
Il training viene svolto utilizzando un \emph{stochastic gradient descent algorithm}, cioè utilizzando
una funzione di ottimizzazione locale scritta sotto forma di funzioni differenziali. La matrice $W$
che rappresenta il \emph{recurrent weights} (il peso degli stati passati) viene calcolata utilizzando la 
\emph{backpropagation throught time algorithm (BPTT)}.
Specificatamente, su RNNLM viene utilizzato un truncate BPTT, la rete cioè processata solo per uno specifico
numero di istanti di tempo.

\subsection{Fase di training}
Nella fase di training i dati in ingresso sono attesi sotto forma di un semplice file ASCII: ogni parola
deve essere separata da spazi e deve essere presente il carattere di
fine linea al termine di ogni frase.
Una volta specificato il corpus di input, il vocabolario viene costruito automaticamente e viene salvato insieme 
al file di modello generato da RNNLM.
Questo significa che, nel caso si voglia limitare il vocabolario, il file di input deve essere modificato in maniera
preventiva, sostituendo
tutte le parole da eliminare con un token speciale (per esempio \textbf{\textless unk \textgreater}).
Oltre al corpus, per completare la fase di training è atteso anche un
file di validazioni dati per regolare il numero di training \emph{epochs} da utilizzare e  il learning rate.
\`E anche possibile fare training di modelli senza utilizzare un file di validazione tramite l'opzione \textbf{-one-iter}.
\subsection{Fase di test}
Una volta effettuato il training il modello può essere utilizzato per valutare dati di test:
l'output di questa valutazione è espresso come \emph{perplexity} (la misura di quanto un modello probabilistico
si avvicina al valore di test) e la 
probabilità $\log_{10}$.
Dato il modello, è possibile effettuare una interpolazione lineare delle probabilità delle parole. L'input atteso
da RNNLM è un file contenente una lista di frasi, ognuna anteposta con un identificatore
numerico univoco, su cui effettuare lo scoring.

\section{Word2Vec}
\label{sec:word2vec}
Storicamente i sistemi di \emph{natural language processing} trattano le parole come simboli atomici e discreti:
per esempio la parola ``gatt'' può essere rappresentata da \textbf{Id123} e la parola ``cane'' come \textbf{Id453}.
Questa codifica è arbitraria e non fornisce alcuna informazione al sistema riguardo la relazione
che può esistere tra due differenti simboli.
Questo significa che il nostro sistema non può utilizzare praticamente nulla di quello che ha imparato
riguardo la parola ``gatto'' quando sta processando i dati riguardanti ``cane'' (per esempio il fatto che siano
entrambi animali, che abbiano quattro zampe, etc.).
Inoltre, rappresentare le parole sotto forma di identificatori univoci porta ad avere una base dati sparsa
con la conseguente necessità di dover ottenere un maggior numero di dati per riuscire a creare un modello
statistico rappresentativo; proprio questa base dati sparsa è il principale problema 
ad essere risolto dalla rappresentazione tramite vettori.

\subsection{Word embedding}
Con il termine \emph{word embedding} intendiamo una rappresentazione di parole in uno spazio vettoriale 
continuo, ovvero un'area in cui le parole
semanticamente simili sono mappate in punti vicini: questo metodo si basa sulla \emph{distributional hypotesis}, 
cioè sul fatto che parole che appaiono in 
un determinato contesto condividono lo stesso significato semantico.

La rappresentazione di parole in uno spazio vettoriale aumenta le performance
di un task di analisi del linguaggio naturale. Una delle prime
applicazioni in questo campo risale al 1986 con la ricerca di Rumelharth, Hinton and Williams.
Questa idea è stata successivamente utilizzata in maniera diffusa, trovando ampia 
applicazione in modelli di ricognizione del parlato,
di traduzione automatica, nonché di numerosi altri tasks.

Recentemente, Mikolov et al. hanno introdotto il modello skip-gram: 
un metodo efficiente per ottenere
rappresentazioni vettoriali di parole provenienti da un grande numero di dati testuali non strutturati.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{Immagini/skipgram.png}
	\caption{modello Skipgram. L'obbiettivo è ottenere vettori di parole capaci di prevedere
	parole simili}
	\label{fig:skipgram}
\end{figure}

Il training, servendosi di un modello Skip-gram  per la creazione di vettori di parole, 
non utilizza moltiplicazioni tra matrici dense, al contrario della maggioranza delle reti 
neurali: in questo modo diventa estremamente efficiente, in quanto una singola macchina può 
calcolare vettori partendo da un testo contenente più di 100 miliardi di parole in meno di un giorno.   

\subsection{Modello Skip-gram}
Nel modello Skip-gram l'obiettivo di training è quello di creare un vettore di parole che
può essere utilizzato per prevedere vocaboli attorno ad una frase o ad un documento. Più formalmente,
data una sequenza di parole $w_1,w_2,w_3,\ldots,w_T$ definita come training set, 
l'obbiettivo del modello è quello di aumentare la
probabilità logaritmica:
\begin{equation}
	\frac{1}{T} \sum_{t=1}^{T} \sum_{-c\leq j\leq c,j\neq0} \log p(w_t+j|w_t)
	\label{eq:prob log}
\end{equation}
dove $c$ è la dimensione del contesto di training (che può essere una funzione basata sulla parola centrale $w_t$).
Utilizzare un $c$ più grande porta ad avere una precisione maggiore. In questo modo, però, si aumenta anche il 
tempo di calcolo. La formulazione standard del modello Skip-gram definisce $p(w_t+j|w_t)$ usando la funzione
softmax:
\begin{equation}
	p(w_O|w_I) = \frac{\exp(v_{w_O} v_{w_I})}{\sum_{w=1}^{W}\exp(v_w v_{w_I})}
	\label{eq:softmax}
\end{equation}

dove $v_w$ and $v'_w$ sono la rappresentazione vettoriale di ``input'' e ``output'' di $w$, mentre $W$ è il numero
di parole nel vocabolario.
Questa formulazione, però, è difficilmente utilizzabile in sistemi reali
perché il costo per il calcolo di $\nabla\log p(w_O|w_I)$
è proporzionale a $W$, che spesso è molto grande: $(10^5-10^7 termini)$.

\subsection{Hierarchical Softmax}
Una approssimazione del softmax computazionalmente più efficiente è il softmax gerarchico. Questo algoritmo
è stato introdotto nell'analisi del linguaggio naturale da Morin e Bengio; il motivo principale
per cui questo algoritmo è più performante rispetto all'algoritmo classico
perchè, per ottenere distribuzione di probabilità, non deve valutare tutti i $W$
 nodi di output della rete neurale, ma solo un numero  $\log_2(W)$ di nodi.

Il softmax gerarchico utilizza una rappresentazione ad albero binario per il \emph{layer} di output 
in cui, come foglie,
troviamo le $W$ parole e, in ogni nodo, la rappresentazione delle probabilità dei nodi figli.
Questo definisce una \emph{random walk} per assegnare la probabilità alle parole.

Più precisamente ogni parola $w$ può essere raggiunta da un determinato percorso partendo dalla radice
dell'albero. Definiamo $n(w,j)$ come il $j$-esimo nodo nel percorso che parte dalla radice fino a $w$ e $L(w)$ come
la lunghezza di questo percorso in modo che $n(w,1) = radice$ e $n(w,L(w)) = w$.
In aggiunta, per ogni nodo interno $n$, definiamo $ch(n)$ come un arbitrario nodo figlio di $n$ e $[x]$ ha il valore $1$ se $x$ è vero e $-1$ altrimenti.
Quindi il softmax gerarchico definisce $p(w_O|w_I)$ come segue:
\begin{equation}
	p(w|w_I) = \prod_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]\cdot v_{n(w,j)} )
	\label{eq:hierarchicalSoftmax}
\end{equation}

dove $\sigma(x) = 1/(1+exp(-x))$. Può essere dimostrato che $\sum_{w=1}^W p(w|w_I) = 1$. Questo implica
che il costo per calcolare $\log p(w_O|w_I)$ e $log p(w_O|w_I)$ è proporzionale a $L(w_O)$ che in media
è più piccolo di $\log W$.
Inoltre, al contrario della formulazione stardand del softmax dove troviamo due rappresentazioni differenti 
$v_w e v_w$ per ogni parola $w$, il softmax gerarchico possiede una sola rappresentazione $v_n$ per ogni nodo $n$
dell'albero binario.

Il tipo di albero utilizzato ha una influenza notevole sulle prestazioni dell'algoritmo, nello specifico
in word2vec è stato utilizzato un albero binario di Huffmann. In questo modo 
abbiamo codici di dimensioni più piccole per le parole usate con maggior frequenza,
determinando un training più rapido.
Inoltre è stato osservato che raggruppare le parole utilizzando la loro frequenza funziona molto
bene come tecnica per aumentare la velocità di calcolo.

\section{Classificatori lineari}
La risoluzione di problemi di classificazione su una mole molto grande di dati è uno dei problemi principali
incontrati in applicazioni quali la classificazione di testi.

Un classificatore lineare, nel campo dell'apprendimento automatico, è diventato una delle tecnologie più
promettenti e più utilizzate: l'obiettivo della 
classificazione lineare è quella di usare le caratteristiche degli oggetti (\emph{Features}) per identificare
a quale classe (o gruppo) appartengono. 
Una classificazione lineare raggiunge questo scopo effettuando una decisione sulla classificazione basata sul valore di una combinazione lineare di caratteristiche. 
Le caratteristiche di un oggetto sono anche conosciute come \emph{features} e nel calcolatore sono rappresentati solitamente come un vettore chiamato vettore delle caratteristiche.

Questi classificatori funzionano molto bene per problemi pratici quali la classificazione di 
documenti e più in generale per problemi che utilizzano un numero molto elevato di \emph{features}

Se definiamo un vettore di \emph{features} con l'identificatore $x^\rightarrow$ allora il punteggio di output
è definito come segue:
\begin{equation}
	y=f(w^\rightarrow \cdot x^\rightarrow = f (\sum_j w_jx_j)
	\label{output score}
\end{equation}

dove $w^\rightarrow$ è un vettore reale pesato (\emph{weight vector}) e $f$ è una 
funzione che converte il prodotto scalare di due vettori nell'input desiderato.
Il \emph{weight vector} viene generato partendo da un inseme di esempi utilizzati come training. Solitamente
la funzione $f$ è una semplice funzione
che effettua il mapping di tutti i valori sopra una certa soglia nella prima classe e gli altri valori nella
seconda classe. Una versione più complessa di $f$ potrebbe fornire una probabilità con cui un particolare
oggetto appartiene ad una certa classe.

L'utilizzo dei classificatori lineari avviene spesso in situazioni in cui la velocità di classificazione
è rilevante, specialmente quando $x^\rightarrow$ è sparso.
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.2]{Immagini/liblinear_esempio.png}
	\caption{Nella figura vediamo come il classificatore $H_1$ e $H_2$ classifichino
	correttamente i punti pieni e i punti vuoti. Potremmo dire, inoltre, che $h_2$
	è un classificatore migliore perché la distanza media tra i vari elementi è costante}
	\label{fig:liblinearEx}
\end{figure}

\subsection{Liblinear}
Liblinear è una libreria opensource che implementa algoritmi di classificazione lineare. Il motivo
per cui è stata scelta come strumento per questa
tesi è principalmente dovuto alla sua estesa documentazione, alla semplicità
d'uso, al suo già ampio utilizzo in ricerche simili e infine per la sua velocità nel calcolo dei risultati. 
In particolare l'ultimo punto si è rivelato fondamentale
per ottenere risultati in tempo ragionevole: in un confronto con un risolutore \emph{SVM} (come \emph{LIBSVM})
i tempi di calcolo vengono ridotti di tre ordini di grandezza (da svariate ore a pochi secondi).

Liblinear supporta due popolari classificatori binari \emph{LR, L2-regularized logistic regression} e 
\emph{SVM L2-loss linear support vector machine}, dato un inseme di coppie
indicizzate $(x_i,y_i),i=1,\dot{},l,x_i\in R^n, y_i \in \{-1,+1\}$ entrambi i metodi risolvono lo stesso
problema di ottimizzazione senza vincoli ma con differenti \emph{loss functions} $\xi(w;x_i,y_i)$:
\begin{equation}
	min_w \frac{1}{2}w^Tw+C\sum_i=1^l\xi(w;x_i;y_i)
	\label{eqn:liblinear1}
\end{equation}

dove $C > 0$ è il parametro di penalità. Per \emph{SVM} le due \emph{loss functions} sono $max(1-y_iw^Tx_i,0)$ e
$max(1-y_iw^Tx_i,0)$. Per \emph{LR} invece abbiamo $\log(1+e^{-y_iw^Tx_i}$

Per quanto riguarda l'utilizzo, il toolkit di Liblinear fornisce due diversi eseguibili: \emph{train} e 
\emph{predict}.
L'eseguibile \emph{train} viene utilizzato per la creazione del modello e acquisisce, come input, un file
ASCII suddiviso per righe. In particolare, per questa tesi, è stato usato come algoritmo 
per il training del modello ottenuto tramite \emph{logistic regression}.
Utilizzando la \emph{regressione lineare} è possibile utilizzare l'eseguibile \emph{predict} in modo 
che fornisca in output le stima delle probabilità per ogni riga.

\chapter{Analisi dei dati}
In questo capitolo verranno descritte inizialmente la raccolta dei dati, 
la loro normalizzazione e la loro struttura. 
Successivamente verranno presentate le tecniche utilizzate per ottenere i risultati.
 Verrà analizzato per primo i risultati ottenuti con \emph{RNNLM}, 
 e a seguire \emph{word2vec} e \emph{Lliblinear}.
 In conclusione verranno mostrati i risultati finali, ottenuti applicando le suddette tecniche.

\section{Raccolta dati}
La raccolta dati è iniziata dal primo Gennaio 2015; i dati raccolti provengono da dodici canali 
Twitch, scelti in modo tale da rappresentare un parte omogenea di informazioni: in particolare sono stati scelti i canali con 
maggiore audience (numero medio di spettatori collegati) dei quattro giochi più popolari, 
secondo le statistiche del mese di Gennaio 2015, della piattaforma Twitch.tv.

È opportuno specificare che i canali, e quindi anche i dati raccolti, 
sono tutti in lingua inglese. Questa scelta è stata determinata dalla 
differenza di popolarità, evidenziata da confronto tra i canali in lingua 
inglese e quelli in lingua italiana: nel mese di Gennaio 2015 la media di 
spettatori per un canale in lingua inglese era di 30.000 persone, 
mentre per un canale italiano si aggirava intorno al centinaio di presenze.

La chat presente sulla piattaforma Twitch utilizza il protocollo IRC, 
per cui Twitch gestisce un server dedicato \footnote{\url{irc://irc.twitch.tv}}. 
Per gli utenti, l’accesso al server di chat è disponibile tramite interfaccia web, 
direttamente dal sito ufficiale. Per eventuali servizi di \emph{backend}
e integrazione, sono disponibili 
delle API ufficiali che offrono la possibilità monitorare e interagire in tempo reale 
con i vari canali, previa registrazione.

Nonostante la piattaforma Twitch fornisca un archivio, accessibile via browser, 
con tutte le trasmissione passate dei suoi canali, 
non è data la possibilità di accedere ai dati storici delle chat. 
Per questo motivo è stato necessario l’utilizzo di un bot IRC che monitorasse in 
modo costante i canali e che provvedesse alla registrazione dei messaggi.

Per la realizzazione del bot è stato utilizzato come base il software opensource Pierc. 
Il suddetto software è stato esteso per permettergli di utilizzare il login e la registrazione 
al server IRC tramite API Twitch. I canali che si vogliono monitorare sono stati 
gestiti tramite file di configurazione testuale e tutti i messaggi registrati salvati 
in un database \emph{MySQL}. La struttura del database è mostrata nella tabella \ref{tab:strutturaDB}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Campo} &\textbf{ Tipo} & \textbf{Descrizione} \\
		\hline
		\hline
		id & int & Id incrementale \\
		\hline
		channel & varchar & Nome canale \\
		\hline
		name & varchar & Nome utente \\
		\hline
		time & datetime & timestamp \\
		\hline
		message & text & messaggio inserito \\
		\hline
		type & varchar & tipo di messaggio IRC \\
		\hline
	\end{tabular}
	\caption{Struttura del database}
	\label{tab:strutturaDB}
\end{table}

E' opportuno notare come si sia scelto di aggiungere, oltre al testo del messaggio, anche 
il timestamp e il canale di appartenenza. Questi dati hanno permesso di effettuare analisi
specifiche considerando solo particolari giornate o dati provenienti solo da un particolare giocatore.

\section{Normalizzazione del corpus}
Uno dei problemi principali incontrati durante l’analisi di dati è stata 
la normalizzazione del corpus. L'analisi di questo tipo di dati non era mai 
stata affrontata prima, quindi è stato impossibile trovare una metodologia già 
applicata a questo tipo di dati.

Inoltre Twitch è una piattaforma relativamente giovane, sviluppatasi negli ultimi 3 anni, 
ma è cresciuta in maniera esponenziale, creando una sottocultura di Internet con frasi 
ricorrenti, giochi di parole e atteggiamenti linguistici nuovi. 
Questo gergo si è talmente esteso da influenzare addirittura altre piattaforme, 
come ad esempio lo streaming su Youtube. 
Questo ha portato alla formazione di particolari tipologie di messaggi, 
suddivise nei modi indicati dalla tabella \ref{tab:messaggiTwitch}  attraverso un'analisi manuale.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Messaggio & Interpretazione \\
		\hline
		\hline
		todo: inserire l'emoticons & emoticon UTF8 \\
		\hline
		todo: inserire la frase  & Frasi contenenti codici UTF8 \\
		\hline
		S U F F E R B O Y S & enfasi \\
		\hline
		\includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png} \includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png} \includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png} \includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png} \includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png} & emoticons multiple \\
		\hline
	\end{tabular}
	\caption{Differenti tipi di messaggio su Twitch}
	\label{tab:messaggiTwitch}
\end{table}

Fortunatamente, oltre a questi messaggi speciali, 
sono presenti una gran quantità di messaggi standard come possiamo vedere nell'esempio seguente:

\begin{verbatim}
so here I am 18 years old in college
korena cheese doesnt exist, its part of normal game Kappa
korean
and my parents are all like, hey what are you going to do with your life
those banes
Moshmonkey seems legit, keep on with the good work bra
it was a grand explosion of blings in the middle of nowhere
\end{verbatim}

Per normalizzare il corpus è stato usato \emph{Twokenizer}, un tokenizzatore sviluppado dalla Carnagie Mellon per gestire dati provenienti da Twitter. 
Si è scelto di utilizzare questo Tokenizzatore perché, a parte i casi particolari indicati 
nella tabella
\ref{tab:messaggiTwitch} l'unica differenza importante che possiamo trovare tra i messaggi di Twitch e
quelli di Twitter sonon gli
hashtag, utilizzati solo in Twitter. 

Twokenizer è stato oppurtunatamete
modificato andando a sostituire tutte le emoticons espresse in caratteri UTF8 con un corrispettivo \emph{token}
tramite una tabella di mapping (una parte di essa è mostrata nella figura \ref{fig:mappingEmo}). In
questo modo si è evitato sia di dover gestire emoticons composte da caratteri multipli sia di gestire
i caratteri UTF8 non supportati, come vedremo, da word2vec.
In secondo luogo si è provveduto nella sostituzione di tutti gli \emph{URL} con il token \textbf{URL} ed infine
sono stati ignorati i simboli \emph{\#} (che indicano i cosidetti hashtag, non presenti su Twitch),
andandoli a trattare come normale segno di punteggiatura.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.3]{Immagini/emoticonsTable.pdf}
	\caption{Parte di tabella di mapping per le emoticons UTF8}
	\label{fig:mappingEmo}
\end{figure}

Vista la mole dei dati raccolti è stato necessario modificare anche il metodo di esecuzione del tokenizzatore, 
che nella sua versione standard, è stato sviluppato per una elaborazione \emph{single thread}.
\`E stato scritto un semplice wrapper per effettuare un \emph{preprocessing} del corpus, 
suddividendolo in $n$ parti distinte, dove con $n$ è indicato il numero di processori presenti 
sulla macchina ed
eseguendo su ciascuna di esse \emph{Twokenizer} in maniera concorrente.
Infine il wrapper effettua l'unione di tutti i dati ottenuti mantenendo lo stesso 
ordinamento del file iniziale.

\section{Analisi Emoticons}
Particolare attenzione è stata posta nell'analisi delle emoticons di \emph{Twitch}: questo tipo di emoticons,
al contrario di quelle viste nel paragrafo precedente, sono parole normali (Es: \emph{kappa, 4head, biblethump}),
queste parole sono trattate in maniera speciale da \emph{Twitch} che le renderizza facendole apparire sotto forma di immagine.
Per esempio la parola \emph{kappa} viene sostituita con 
\includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/kappahd.png}, 
\emph{4head} con \includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/4head.png} e 
\emph{biblethump} con
\includegraphics[height=0.4cm, width=0.4cm]{Immagini/Emoticons/biblethump.png} 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{Immagini/TwitchEmotes.jpg}
	\caption{Screenshot dell'interfaccia browser di Twitch che mostra le emoticons renderizzate
	come appaiono all'utente finale.}
	\label{fig:TwitchEmotes}
\end{figure}

Le emoticons fornite da twitch sono 120, tramite un analisi su tutto il corpus raccolto si è scelto di fare un analisi
manuale di solo le 25 emoticons usate con maggior frequenza. Analizzando alcune centinaia di messaggi 
per ogni emoticon
si è data una classificazione alla tipologia di frasi in cui l'emoticons è utilizzata. Il risultato è
quello mostrato nella tabella \ref{tab:emoticons1}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Nome Emoticon & Significato & Polarity \\
\hline
\hline
4Head & sarcasmo & positiva \\
\hline
anele & argomenti riguardati gli USA & neutra \\
\hline
babyrage & insofferenza & negativo \\
\hline
biblethump & disappunto & negativo \\
\hline
brokeback & sarcasmo & positiva \\
\hline
dansgame & critica & negativo \\
\hline
datsheffy & battute sulle forze dell'ordine & non considerato \\
\hline
elegiggle & sarcasmo & positiva \\
\hline
pogchamp & spam & non considerato \\
\hline
residentsleeper & noia & negativo \\
\hline
smorc & frasi incentrate su un particolare gioco & neutra \\
\hline
swiftrage & rabbia & negativo \\
\hline
trihard & impegno &  neutro \\
\hline
wutface & spam & non considerato \\
\hline
failfish & disappunto & negativo \\
\hline
frankerz & spam & non considerato \\
\hline
heyguys & annunci e saluti & positiva \\
\hline
kappa & sarcasmo & positiva \\
\hline
kappapride & sarcasmo & positiva \\
\hline
kreygasm & sarcasmo & positiva \\
\hline
mrdestructoid & viewbotting & neutre \\
\hline
opieop & commenti sul cibo & neutra \\
\hline
osbury & offese & negativo \\
\hline
osrob & spam & non considerato \\
\hline
pjsalt & disappunto & negative \\
\hline
\end{tabular}
\end{center}
\caption{Analisi delle emoticons}
\label{tab:emoticons1}
\end{table}

Analizzando la tabella è facile notare come sia possibile dividere le emoticons in quattro classi separate:
\begin{itemize}
\item \textbf{Positiva}: rappresenta frasi ironiche, sarcastiche e frasi che esprimono un apprezzamento sul contenuto del canale.
\item \textbf{Negativa}: rappresenta un giudizio negativo sul contenuto del canale o su gli argomenti trattati in chat.
\item \textbf{Neutra}: non hanno una polarità definita, in questa classe troviamo emoticons, per esempio \emph{opieop}, utilizzate in frasi che parlano di 
cibo ``I'm getting an hamburger now opieop'', frasi che quindi sono scollegate dall'argomento dello streaming o della chat. 
In questa classe troviamo anche emoticons come \emph{mrdestructdroid}, che viene utilizzato principalmente per indicare il cosidetto ``viewbotting'' 
cioè l'utilizzo di software che crea connessioni fittizie allo streaming in modo da incrementare il numero di visualizzazioni 
e quindi il ranking dello stream stesso (andando ad aumentare gli eventuali guadagni provenienti dalle pubblicità). 
L'analisi di questo comportamento, anche se molto interessante, esula dagli argomenti trattati in questa tesi pertanto viene ignorato.
\item \textbf{Non considerata}: in questa classe troviamo le emoticons che non esprimono alcun giudizio, sono utilizzate spesso solo per creare ``confusione'' nella chat.
Per esempio emoticons come \emph{osrob} o \emph{frankerz} vengono utilizzate in frasi senza significato, spesso contenenti solo la stessa emoticons ripetuta,
in modo da generare una enorme quantità di messaggi in chat. Questo comportamento, come avviene talvolta anche nei forum, viene utilizzato 
soltanto per impedire una discussione in chat e per creare disordine: è nel vero senso della parola solo \emph{spam}. 
Per quanto concerne la presente tesi,
queste frasi sono state mantenute nel corpus, ma non gli è stata data alcun tipo di polarità: sono considerate frasi neutre.
\end{itemize}

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Positive & Negative & Non considerate/Neutre \\
\hline
4Head & babyrage & datsheffy \\
\hline
anele &  biblethump &  pogchamp \\
\hline
brokeback &  dansgame & smorc \\
\hline
elegiggle & residentsleeper &  swiftrage \\
\hline
heyguys & failfish & trihard \\
\hline
kappa & pjsalt & wutface \\
\hline
kappapride & osbury & frankerz \\
\hline
kreygasm &  & keepo \\
\hline
 & & mrdestructoid \\
\hline
& & opieop \\
\hline
& & osrob \\
\hline
\end{tabular}
\end{center}
\caption{Lista di emoticons raggruppata per polarity}
\label{tab:emoticons2}
\end{table}

Infine una delle caratteristiche emerse da questa prima analisi dei dati è il fatto che possiamo dividere i messaggi in due diversi gruppi: i messaggi che esprimono un parere
riguardo eventi accaduti nel video (quindi esprimono un parere su i contenuti) e i \emph{meta}-messaggi cioè messaggi che esprimono opinioni sullo stream stesso (es: ``la qualità video
è pessima'', ``oggi non si sente l'audio'') oppure su fatti che esulano dai contenuti del video (es. ``sto male, ho preso l'influenza''). Nelle analisi svolte le due categorie
di messaggi sono state trattate in egual modo e non sono state differenziate.
  
\section{Strumenti di analisi}
Per lo svolgimento della tesi sono stati utilizzati i vari strumenti descritti nel capitolo precedente,
nel dettagglio e' stata utilizzato \textbf{RNNLM} versione 0.4b \footnote{\url{https://goo.gl/e9AxGR}}, 
\textbf{word2vec} revision 42 \footnote{\url{http://goo.gl/IWUQoE}}
a cui e' stata applicata la patch per i \emph{paragraph vectors}, \textbf{Liblinear} versione 2.1 \footnote{\url{http://goo.gl/4daeR3}}.

Per automatizzare e integrare il processo di test, training, analisi e integrazione tra i vari strumenti
sono stati sviluppati diversi programmi e script utilizzando svariati linguaggi: \emph{BASH, Python} (sia 
la versione 2.7 che 3.3) e \emph{C\#}.

Tutto il software scritto per questa tesi e' disponibile nel \emph{repository Git} al seguente indirizzo:
\footnote{\url{https://github.com/ManofWax}}.

\subsection{RNNLM}
\label{sss:rnnlm}
Seguento le ricerche di Mikolov (2014) %todo
e' stato suddiviso il corpus utilizzando come elemento discriminante le 25 emoticons individuate in precedenza.
In questo modo
sono stati creati 25 file differenti, in ogni file sono presenti tutte le frasi in cui e' contenuta una determinata
emoticons, nei file e' stata mantenuta la stesso ordinamento delle frasi originali.
Nel caso in cui, in una stessa frase, sia presente piu' di una emoticon essa e' stata inserito in due o piu' file
differenti.

Effettuata questa prima suddivisione sono state rimosse tutte le emoticon dai file ottenuti, ed e' stata effettuata 
una ulteriore divisone in quelli che possiamo definire come file di \emph{train} e di \emph{test}.
I file di \emph{train} contengono 25000 righe selezionate in maniera casuale tra tutte quelle del file originale,
il file di test, invece, e' composto da 15000 righe sempre selezionate casualmente tra quelle del file originale.
Particolare attenzione e' stata posta nell'evitare che righe selezionate per il file di \emph{train}
non vengano utilizzate anche in quello di \emph{test}.

Il modello, per ogni file, e' stato generato utilizzando la seguente configurazione:

\begin{verbatim}
head -n 24800 $dir > $dir.train
tail -n 200 $dir > $dir.valid

./rnnlm -rnnlm $dir.model -train $dir.train -valid $dir.valid \
        -hidden 50 -direct-order 3 -direct 200 -class 100 -bptt 4 \
        -bptt-block 10 -binary
\end{verbatim}

Successivamente sono state prese 4 emoticons normalmente usate per esprimere un parere positivo e 6 per esmprimere un parere negativo e confrontate fra loro.
Il confronto e' stato effettuato prendendo i modelli generati da i file di \emph{train}
delle due differenti emoticon e i due file di \emph{test}. I file di \emph{test}
vengono uniti in modo da avere un file di test unico e utilizzando la funzione \textbf{n-best scoring} di RNNLM
su questo file vengono gerenati due file risultati differenti, uno basato sul
modello positivo e uno sul modello negativo.
\begin{verbatim}
awk 'BEGIN{a=0;}{print a " " $0; a++;}' < test.txt > test-id.txt
./rnnlm -rnnlm model-pos -test test-id.txt -nbest > pos-score
./rnnlm -rnnlm model-neg -test test-id.txt -nbest > neg-score
\end{verbatim}

Infine vengono uniti e processati i due file dei risultati, se il rapporto tra il risultato ottenuto
dal modello negativo con quello positivo e' maggiore di $1$ allora alla frase viene attribuito
un valore positivo altrimenti gli viene assegnato un valore negativo.

Nelle tabelle sono rappresentate le percentuali di \emph{accuracy} ottenute:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{pos/neg} & Babyrage & Biblethump & Dansgame & Failfish & notlikethis & wutface \\
\hline
4Head & 80.016 &  78.322 & 76.432 & 77.985 & 80.543 & 78.202 \\
\hline
Elegiggle & 81.372 & 80.696 & 78.794 & 77.454 & 80.768 & 81.288 \\
\hline
Kappa & 80.394 & 78.548 & 78.906 & 78.12 & 82.162 & 81.373 \\
\hline
Kreygasm & 82.944 & 80.964 & 78.388 & 80.228 & 82.608 & 81.114 \\
\hline
\end{tabular}
\end{center}
\caption{Confronto emoticons positive e negative}
\label{tab:rnnlmTest1}
\end{table}

Nello steso modo sono state confrontate alcune emoticon positive fra di loro:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
pos/pos & 4Head & Elegiggle & Kappa & Kreygasm \\
\hline
\hline
4Head & 49.87 &  72.42 & 73.68 & 77.37 \\
\hline
Elegiggle & 72.126 & 50.37 & 78.58 & 81.98 \\
\hline
Kappa & 73.97 & 78.76 & 49.70 & 78.178 \\
\hline
Kreygasm & 77.41 & 82.8 & 79.178 & 49.62 \\
\hline
\end{tabular}
\end{center}
\caption{Confronto emoticons positive}
\label{tab:rnnlmTest2}
\end{table}

In questo modo è facile notare che non solo RNNLM riesce a distinguere con alta probabilita'  tra frasi provenienti da emoticons positive e quelle da emoticons negative. 
Ma riesce anche a distinguere con elevata precisione le differenze tra due diverse emoticons positive.

%\afterpage{
%\clearpage
%\thispagestyle{empty}
%\begin{landscape}
%\centering
%\begin{table}[h]
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
% & 4head & anele & babyrage & biblethump & brokeback & dansgame & datsheffy & elegiggle & failfish & frankerz & heyguys & kappapride & kappa & keepo & kreygasm & mrdestructoid & opieop & osbury & osrob & pjsalt & pogchamp & residentsleeper & smorc & swiftrage & trihard & wutface \\
%4head &  49.638\% &  85.668\% &  82.05\% &  79.074\% &  76.462\% &  77.964\% &  90.016\% &  71.952\% &  77.076\% &  85.644\% &  85.906\% &  76.828\% &  74.556\% &  73.956\% &  77.214\% &  87.244\% &  79.09\% &  97.264\% &  94.782\% &  81.212\% &  73.688\% &  82.892\% &  &  &  & \\
%anele &  &  49.962\% &  90.482\% &  88.012\% &  87.132\% &  88.236\% &  92.522\% &  88.9\% &  89.868\% &  87.402\% &  91.008\% &  85.584\% &  85.946\% &  85.866\% &  86.542\% &  90.804\% &  85.872\% &  97.146\% &  97.204\% &  89.542\% &  87.364\% &  91.866\% &  88.7\% &  90.362\% &  85.904\% &  88.028\%\\
%babyrage &  &  &  50.094\% &  82.066\% &  84.308\% &  84.884\% &  93.334\% &  84.246\% &  84.112\% &  88.434\% &  90.384\% &  84.706\% &  83.196\% &  83.19\% &  84.616\% &  90.81\% &  85.408\% &  97.682\% &  96.252\% &  87.872\% &  83.644\% &  88.4\% &  87.298\% &  85.03\% &  89.026\% &  86.298\%\\
%biblethump &  &  &  &  50.022\% &  83.162\% &  81.068\% &  91.722\% &  83.44\% &  82.318\% &  85.338\% &  88.02\% &  80.874\% &  79.278\% &  79.388\% &  81.566\% &  88.616\% &  82.164\% &  97.3\% &  95.134\% &  86.636\% &  80.894\% &  86.444\% &  86.02\% &  83.218\% &  86.318\% &  80.704\%\\
%brokeback &  &  &  &  &  50.212\% &  80.112\% &  91.712\% &  79.862\% &  77.534\% &  87.13\% &  87.156\% &  80.892\% &  79.338\% &  78.786\% &  78.338\% &  89.308\% &  81.09\% &  96.934\% &  96.326\% &  84.73\% &  79.886\% &  84.766\% &  83.618\% &  86.012\% &  84.808\% &  81.72\%\\
%dansgame &  &  &  &  &  &  50.188\% &  91.97\% &  81.268\% &  76.622\% &  87.676\% &  88.26\% &  81.818\% &  80.72\% &  80.136\% &  79.93\% &  89.924\% &  82.37\% &  97.694\% &  94.892\% &  86.99\% &  78.464\% &  85.474\% &  85.972\% &  82.058\% &  86.12\% &  76.998\%\\
%datsheffy &  &  &  &  &  &  &  49.976\% &  90.514\% &  92.098\% &  92.186\% &  93.822\% &  91.234\% &  90.942\% &  90.406\% &  91.39\% &  93.002\% &  89.964\% &  98.184\% &  97.336\% &  93.192\% &  90.792\% &  93.486\% &  92.362\% &  92.488\% &  91.832\% &  92.448\% \\
%elegiggle &  &  &  &  &  &  &  &  49.942\% &  78.47\% &  89.74\% &  89.276\% &  82.63\% &  79.42\% &  79.698\% &  82.324\% &  89.884\% &  82.878\% &  97.276\% &  94.734\% &  85.352\% &  79.718\% &  85.376\% &  86.694\% &  85.15\% &  87.324\% &  84.292\%\\
%failfish &  &  &  &  &  &  &  &  &  49.9\% &  89.344\% &  89.636\% &  83.634\% &  78.926\% &  78.448\% &  81.096\% &  90.666\% &  83.182\% &  97.756\% &  95.738\% &  86.544\% &  80.558\% &  84.358\% &  84.07\% &  85.022\% &  87.616\% &  81.916\%\\
%frankerz &  &  &  &  &  &  &  &  &  &  49.89\% &  89.774\% &  83.898\% &  85.302\% &  82.472\% &  87.604\% &  89.27\% &  84.768\% &  97.106\% &  97.068\% &  86.454\% &  87.176\% &  90.716\% &  88.694\% &  90.114\% &  86.466\% &  88.002\%\\
%heyguys &  &  &  &  &  &  &  &  &  &  &  49.85\% &  86.042\% &  87.862\% &  86.734\% &  85.712\% &  92.036\% &  87.23\% &  97.426\% &  97.166\% &  90.164\% &  86.326\% &  91.948\% &  91.41\% &  91.314\% &  89.478\% &  88.52\%\\
%kappapride &  &  &  &  &  &  &  &  &  &  &  &  50.264\% &  &  75.91\% &  79.28\% &  88.798\% &  80.438\% &  97.046\% &  95.992\% &  84.532\% &  79.836\% &  87.022\% &  85.314\% &  85.18\% &  84.066\% &  82.592\%\\
%kappa &  &  &  &  &  &  &  &  &  &  &  &  76.268\% &  49.902\% &  71.292\% &  78.71\% &  87.22\% &  78.298\% &  97.572\% &  95.2\% &  84.958\% &  77.112\% &  84.758\% &  82.486\% &  84.074\% &  84.852\% &  82.796\%\\
%keepo &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.368\% &  78.54\% &  87.052\% &  77.286\% &  97.492\% &  96.352\% &  83.93\% &  78.416\% &  85.408\% &  82.802\% &  84.166\% &  83.648\% &  82.418\%\\
%kreygasm &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.004\% &  87.99\% &  81.662\% &  97.566\% &  95.404\% &  85.918\% &  71.374\% &  85.754\% &  84.664\% &  83.842\% &  83.504\% &  81.59\%\\
%mrdestructoid &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.868\% &  89.116\% &  97.898\% &  97.268\% &  91.098\% &  89.688\% &  91.228\% &  90.71\% &  90.454\% &  89.44\% &  90.524\%\\
%opieop &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.73\% &  97.68\% &  95.226\% &  86.306\% &  80.384\% &  87.346\% &  85.158\% &  87.116\% &  83.586\% &  84.342\%\\
%osbury &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.96\% &  98.856\% &  97.538\% &  97.518\% &  98.064\% &  97.954\% &  97.672\% &  97.298\% &  97.424\%\\
%osrob &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.988\% &  96.822\% &  96.118\% &  97.134\% &  95.87\% &  96.11\% &  96.548\% &  96.582\%\\
%pjsalt &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.068\% &  85.194\% &  89.502\% &  88.208\% &  89.998\% &  87.598\% &  87.048\%\\
%pogchamp &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.348\% &  85.058\% &  84.184\% &  81.532\% &  85.13\% &  80.16\%\\
%residentsleeper &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.17\% &  88.79\% &  88.07\% &  89.22\% &  87.948\%\\
%smorc &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.91\% &  86.068\% &  87.62\% &  86.966\%\\
%swiftrage &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.776\% &  89.082\% &  85.906\%\\
%trihard &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  50.014\% &  84.77\%\\
%wutface &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  49.804\%\\
%\end{tabular}
%\caption{Confronto emoticon globale}
%\label{tab:rnnlmTest3 }
%\end{table}
%\end{landscape}
%\clearpage
%}

\subsection{Word2Vec}
\label{sss:word2vec}
Utilizzando lo stesso procedimento effettuato per i test con \emph{RNNLM} sono stati utilizzati i file di \emph{train} e di \emph{test}
per svolgere gli stessi test di \emph{accurancy} tra le frasi provenienti da due differenti emoticon.

\`E importante sottolineare che, nonostante venga considerato un sottoinsieme dei dati originali, i vettori sono stati generati utilizzando 
l'intero corpus. I vettori sono stati generati utilizzando il seguente comando:

\begin{verbatim}
./word2vec -train vec-id.txt -output vectors.txt -cbow 0 \
           -size 100 -window 10 -negative 5 \
           -hs 0 -sample 1e-4 -threads 40 -binary 0 \
           -iter 20 -min-count 1 -sentence-vectors 1
\end{verbatim}

Sono state effettuate alcune prove cambiando la lunghezza dei vettori: sono stati utilizzati vettori da 100, 200 e 300 elementi e anche prove
utilizzando sia l'algoritmo c-bow che skip-gram. 
Non sono state osservate particolari differenze nei risultati, per questo motivo si è scelto di utilizzare vettori da 100 elementi e l'algoritmo skip-gram.

Per quanto riguarda la valutazione del risultato e' stato utilizzato \emph{Liblinear}, in particolare e' stata usata l'algoritmo
di \emph{logistic regression} per calcolare l'\emph{accurancy} sul file di \emph{test}:

\begin{verbatim}
./train -s 0 train.txt model.logreg
./predict -b 1 test.txt model.logreg out.logreg
\end{verbatim}

Infine, utilizzando lo stesso set di dati del paragrafo \ref{sss:rnnlm}, e' stata calcolata l'\emph{accurancy} tra coppie di emoticon positive e negative:

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
pos/neg & Babyrage & Biblethump & Dansgame & Failfish & notlikethis & wutface \\
\hline
\hline
4Head & 67.01 &  69.322 & 68 & 67 & 65.5 & 70.202 \\
\hline
Elegiggle & 69.37 & 71.696 & 70.74 & 67.54 & 69.7 & 68.28 \\
\hline
Kappa & 70.94 & 69.548 & 69.906 & 68.12 & 68.162 & 71.37 \\
\hline
Kreygasm & 69.44 & 71.96 & 70.33 & 72.43 & 67.76 & 71.32 \\
\hline
\end{tabular}
\end{center}
\caption{Confronto emoticon positive e negative}
\label{tab:liblinearTest1}
\end{table}

Anche in questo caso, come e' avvenuto con \emph{RNNLM}, possiamo notare come \emph{word2vec} riesca a distinguere tra le varie emoticon con precisione di circa il $70\%$.
Confrontando i valori si puo' vedere come la precisione sia comunque inferiore di quella calcolata con \emph{RNNLM} di circa 10 punti percentuali.

%Sono stati effettuati anche alcuni test utilizzando tutti i vari algoritmi disponibili da liblinear; questo test è stato svolto utilizzando tutti i parametri di default e testato con tutti gli algoritmi la seconda colonna rappresenta i risulati ottenuti con -c 2
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|l|c|r|}
%\hline
%Algoritmo & c  default & -c 2 \\
%\hline
%\hline
%0 & 66.9 & 66.88\\
%\hline
%1 & 66.93 & 66.95\\
%\hline
%2 & 66.93 & 67.14\\
%\hline
%3 & 67.16 & 67.24\\
%\hline
%4 & 67.15 & 66.93\\
%\hline
%5 & 66.94 & 66.90\\
%\hline
%6 & 66.9 & 66.84\\
%\hline
%7 & 66.90 & 66.89\\
%\hline
%11 & 66.93 & 66.93\\
%\hline
%12 & 66.47 & 66.63\\
%\hline
%13 & 65.188 & 67.02\\
%\hline
%\end{tabular}
%\end{center}
%\caption{test liblinear}
%\label{tab:liblineraTest1}
%\end{table}
%
%Il parametro C è stato ricercato utililzzando la funzionalità di liblinear -C: ./train -s 0 -C train.txt . Il risultato ottenuto è stato un c pari a 2. Un ulteriore test utilizzando l'algoritmo 2 e modificando i c da 1 a 4 non ha portato alcuna differenza nei valori di precisione.

Utilizzando l'algoritmo \emph{t-SNE}, cioe' un algoritmo di \emph{Distributed Stochastic Neighbor Embedding }, e' stato possibile
ridurre le dimensioni dei vettori generati tramite \emph{word2vec} e generare un grafico cartesiano rappresentante la ``distanza''
tra le varie emoticon.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Immagini/tsne.png}
	\caption{Grafico rappresentante i vettori di parole.}
	\label{fig:tSNE}
\end{figure}

\section{Risultati utilizzando entrambi i modelli}
I test conclusivi per la classificazione di polarità tra due diverse emoticon
sono stati effettuati eseguendo sia \textbf{RNNLM} e poi successivamente \textbf{word2vec e Liblinear} sullo stesso set di dati.

Come citato nel paper
%todo
sono state confrontate le probabilita' logaritmiche di entrambi i modelli utilizzando l'interpolazione lineare.
Piu' formalmente, e' stata definita la probabilita' complessima come la media geometrica pesata
dei modelli:
\begin{equation}
	p(y=+1|x)=\prod p^k(y=+1|x)^{\alpha_k}, con \alpha_k > 0
	\label{eq:media}
\end{equation}
La ricerca di $\alpha$ e' stata effettuata utilizzando un \emph{brute force grid search}, quantificando
i valori dei coefficenti nell'intervallo $[0,1]$ in incrementi di $0,1$.
Questa ricerca viene controllata tramite un corpus di validazione in modo da evitare l'overfitting.

Da come è stato possibile leggere nei capitoli precedenti sono stati eseguiti due test: il primo utilizzando i dati della tabella  \ref{tab:rnnlmTest1}, mentre il secondo è stato svolto utililzando tutte le emoticons positive 
e negative descritte nella tabella \ref{tab:emoticons1}. 
I test sono stati sempre svolti con un corpus di training di 12500 righe per file (file positivo e negativo) scelte casualmente tra tutto il corpus e un test di 25000 per file. I sentence vectors sono stati generati utililzando l'intero corpus di emoticons positive, negative e neutre non limitato al numero di righe prese in considerazione per i file di training e test.

Per rapportare i dati ottenuti da \emph{RNNLM} con quelli di \emph{word2vec} e' stato utilizzato lo stesso procedimento
descritto da 
I risultati ottenuti sono i seguenti:
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& rnnlm & word2vec & totale \\
\hline
no ripetizioni 1 & 72.53 & 66.50 & 75.85  \\
\hline
no ripetizioni 2 & 73.34 & 67.50 & 74.85  \\
\hline
no ripetizioni 3 & 72.13 & 66.78 & 75.01  \\
\hline
standard 1 & 79.52 & 67.01 & 79.89 \\
\hline
standard 2 & 80.33 & 69.23 & 81.01 \\
\hline
standard 3 & 78.92 & 68.33 & 80.90 \\
\hline
\end{tabular}
\end{center}
\caption{test liblinear}
\label{tab:test1}
\end{table}

In questo caso sono stati ripetuti 3 volte i test, utilizzando un set di dati di training e test diverso per ogni test. In particolare è stato effettuato un set test rimuovendo le ripetizioni nelle stasse frase. Questo meccanismo, oltre ad alterare il corpus originale, porta a valori di precisione leggermente inferiori e quindi è stato scartato dai successivi test.
\begin{es}
Eliminazione di ripetizione: In una frase contenente le seguenti parole:  ``Good Game Kappa Good Game Kappa Good Game Kappa`` è stata ridotta eliminando le parole ripetute ad una frase del tipo ``Good Game Kappa``.
\end{es}

\subsection{Analisi di più emoticons}
\`E stato suddiviso il corpus in due parti: una parte contenente tutte le frasi positive: cioè quelle contenenti una delle emoticon citate nella tabella \ref{tab:rnnlmTest1} e quello negativo costituto 
in egual modo ma utilizzando le frasi marcate con emoticon negative. 
Questi due set di dati sono statati randomizzati in modo che le frasi provenienti dalle differenti emoticon venissero considerate con uguale probabilità. Sono state estratte 25000 righe da entrambi i file ed, 
effettuando lo stesso procedimento utilizzato nei capitoli \ref{ch:rnnlm} e \ref{ch:liblinear}, è stato fatto il training sia con \emph{RNNLM} che con \emph{Liblinear}, 
dopo aver processato i vettori (costruiti con l'intero corpus positivo, negativo e neutro).
I test sono stati effettuati utilizzando altre 25000 righe provenienti da entrambi i file positivi e negativi (tutte frasi differenti da quelle utilizzate per il training). Il risultato ottenuto è riportato nella tabella \ref{tab:test2}.
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& rnnlm & c liblinear & totale \\
\hline
\hline
Test 1 & 54.20 & 52.32 & 53.30 \\
\hline
Test 2 & 53.32 &  51.50 &  53.85  \\
\hline
Test 3 & 53.20 & 52.01 &  51.89 \\
\hline
\end{tabular}
\end{center}
\caption{test liblinear}
\label{tab:test2}
\end{table}

I risultati cosi bassi sono dovuti al fatto che, anche se concettualmente le emoticons positive esprimono una polarità positiva in realtà la costruzione delle frasi associate sono molto diverse. 
In altre parole il sistema si ``confonde`` tra le varie emoticons positive e quelle negative. Come indicato nelle tabelle del capitolo \ref{ch:rnnlm} è facile vedere come l'algoritmo riecsca a differenziare molto 
bene anche tra emoticons che esprimono lo stesso giudizio (positivo o negativo). Utilizzando frasi provenienti da tutte queste emoticon il training considera della stessa ``distanza`` 
sia le eventuali emoticons positive che quelle negative e riesce a distinguere le frasi in maniera molto meno precisa.

\subsection{Miglioramento risultati analisi con più emoticons}
Sono state fatte numerose prove per migliorare la precisione del risultato, la soluzione utilizzata, cioè quella che ha raggiunto un livello di accurancy più alta è stata la seguente:
Sono state suddivise le frasi per ogni emoticons considerata: quindi il corpus è stato suddiviso in 7 file positivi e 8 negativi. Prendendo 12500 righe casuali da ognuno di questi file è stato generato un modello per ogni singolo file utilizzando rnnlm e liblinear.(I vettori utilizzati per liblinear sono stati sempre costruiti nel solito modo tenendo conto dell'intero corpus quindi anche delle frasi neutre). In questo modo sono stati ottenuti 8 modelli per le emoticons negative e 7 per le emoticons negative.
Per il test sono state prese 25000 righe di emoticons positive e altrettante negative in maniera casuale tra tutte quelle delle emoticons positive e negative ed in questo modo è stato creato il file di test. Questo file di test è stato confrontato con ogni combinazione di modelli positivo-negativo generati e sono stati salvati tutti i risultati parziali.
\begin{es}
Consideriamo le emoticon trattate:
\begin{itemize}
\item Positive: kappa, 4head, elegiggle, kappapride, kreygasm, heyguys, anele
\item Negative: residentsleeper, pjsalt, wutface, notlikethis, failfish, biblethump, dansgame, babyrage
\end{itemize}
Il file di test viene processato usando come modello di training ogni confronto tra emoticons, quindi prima ottengo il risultato processando il file con il modello kappa-residentsleeper, poi kappa-pjsalt, kappa-wutface etc. In questo modo ottengo 56 risultati differenti che rappresentano tutti le possibili combinazioni di confronti tra emoticons positive con quelle negative.
\end{es}

In questo modo sono state ottenui 56 risultati parziali per ogni riga del file di test, questi risultati parziali sono stati confrontati facendo la sommatoria di quelli che hanno dato esito positivo con quelli che hanno dato esito negativo, se è stata ottenuta la maggioranza di risultati positivi allora alla frase è stato attribuito un punteggio positivo, se invece la maggioranza risultate negativo è stato attribuito un valore negativo. In caso di pareggio il valore assegnato è neutro.
Utilizzando questa procedura i valori ottenuti sono paragonabili a quelli del  test singolo fatto in precedenza:
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& rnnlm & liblinear & totale \\
\hline
\hline
Test 1 & 70.20 & 61.32 & 72.30 \\
\hline
Test 2 & 72.53 & 60.40 & 73.85  \\
\hline
Test 3 & 72.52 & 62.01 & 73.89 \\
\hline
\end{tabular}
\end{center}
\caption{test liblinear}
\label{tab:test3}
\end{table}

\section{Baseline}
Come baseline è stato effettuato, sugli stessi dati utilizzati nel test precendente, un BOW utilizzando Weka e processando i risultati utilizzando liblinear con gli stessi parametri utilizzati per per word2vec. I risultati sono indicati nella tabella \ref{tab:baseTest1}
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|}
\hline
& risultato  \\
\hline
\hline
Test 1 & 62.22 \\
\hline
Test 2 & 63.56 \\
\hline
Test 3 & 63.41 \\
\hline
\end{tabular}
\end{center}
\caption{test bow}
\label{tab:baseTest1}
\end{table}

Si può vedere che utilizzando il bow i risultati sono circa di 10 punti percentuali inferiori a quelli ottenuti utilizzando rnnlm, word2vec e liblinear
\section{Analisi dell'andamento giornaliero}
Sono state analizzate le chat e i video prodotti da alcuni streamer nell'intero arco della giornata. I dati sono stati processati dapprima utilizzando il tokenizzatore e poi rimuovendo le emoticons. L'intera giornata è stata suddivisa in intervalli di 5 minuti, per ogni intervallo sono state calcolate le somme di tutte le frasi positive, negative e neutre. Questo a permesso di ottenere un grafico dell'andamento della giornata.
E' stato calcolato anche anche uno stesso andamento utilizzando lo stesso corpus, questa volta senza aver eliminato le emoticons utilizzando un semplice algoritmo che considera solo il numero di emoticons presenit in ogni frase. ES: in una frase ci sono 3 emoticons positive e 1 negativa allora assumo che la frase sia positiva.
Come test iniziale sono stati analizate tre giornate di tre differenti persone:
\newline
\newline
Per ora i dati sono centenuti nei tre excel: il primo foglio rappresenta i dati generati utilizzando l'algoritmo rnnlm, word2vec, liblinear, il secondo foglio rappresenta i dati generati solo dal calcolo delle emoticons e nel terzo fogli ci sono i 3 grafici: l'andamento utilizzando l'algoritmo, l'andamento utilizzando solo le emoticons e infine il confronto tra l'andamento generato dall'algoritmo e dalle sole emoticons.

Vediamo il file \emph{1\_6\_amazhs.xlsx}, durante questa giornata tutti i commenti in chat sono riferiti ad eventi nello stream (vittorie, sconfitte) e l'utilizzo estremamente elevato di emoticons ha portato ad una eguagliana piuttosto elevata dai dati ottenuti solamente dall'analisi delle emoticons. I vari picchi sono stati verificati manualmente e rappresentavano correttamente eventi positivi o negativi

Nei due file successivi \emph{1\_4\_destiny.xlsx} e \emph{1\_7\_massansc.xlsx} vediamo come l'analisi delle sole emoticons non è più sufficiente. La divergenza è dovuta al fatto che l'utilizzo in chat di emoticons è relativamente basso rispetto al numero di messaggi, la polarity generata con rnnlm/word2vec tiene conto anche dei messaggi senza emoticons. Con un controllo manuale nei vari picchi è risultato che la polarity rilevata da rnnlm/word2vec è effettivamente quella corretta, in più viene rilevata correttamente sia la polarità degli eventi dello stream che dei meta-eventi (disconnessioni, problemi tecnici etc. vedi capitolo \ref{ch:sviluppi}

\chapter{Conclusioni e sviluppi futuri}
\label{ch:sviluppi}
\begin{itemize}
\item Nell'analisi delle emoticons multiple invece di fare una semplice ``binary decision`` cioè contare il numero di volte che è stato assegnato un valore positivo e negative e assegnare la polarità a seconda del risultato della differenza tra i due valori si potrebbe utilizzare liblinear per creare un modello intermedio per i 56 risultati usato poi per classificare i risultati generati dal file di test.
\item Nei grafici finali aggiungere anche il grafico dell'andamento della polarity calcolato con la baseline (bow + liblinear)
\item Analisi della meta-polarità: facendo verifice manuali su i dati finali ottenuti è risultato che si potrebbe fare un'analisi in due livelli: il primo livello rappresenta la polarità del contenuto dello stream, per esempio se nello stream avviene un evento positivo (es. una vittoria) o negativo (es. una sconfitta) la chat reagisce di conseguenza aumentado il numero di commenti positivi o negativi.
Si è visto che è presente anche un secondo livello, un meta-livello, dove le frasi scritte in chat (quindi anche la polarita) è riferita ad eventi che esulano dai contenuti dello stream, ma l'oggetto della discussione riguarda lo stream stesso (es: ``Stream quality sucks!'' [negativa] oppure ``The song yoùre playing is amazing!'' [positiva], ``you are cheating!'' [negativa] etc.).
\end{itemize}

\bibliographystyle{apalike}
\bibliography{bibliografia}

\end{document}
